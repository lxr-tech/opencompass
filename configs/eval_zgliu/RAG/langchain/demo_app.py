import random
import json
import os.path
import requests
import numpy as np
from typing import Iterable, List, Tuple, Dict, Any
import streamlit as st
import jsonlines
from sentence_transformers import SentenceTransformer
from faiss import write_index, read_index
import faiss
import jieba
from typing import List, Dict
from rank_bm25 import BM25Okapi
import nltk
from nltk.tokenize import word_tokenize
import os
import json
import random
import logging
from collections import defaultdict
from typing import Any
from openai import OpenAI
from tqdm import tqdm, trange

nltk.download('punkt')

logger = logging.getLogger(__name__)

model_path = 'jinaai/jina-embeddings-v2-base-zh'

def build_faiss_index(embedding_model, data_path):
    datas = []
    print('Begin loading data:')
    with open(data_path, 'r', encoding='utf-8') as file:
        for line in file:
            json_data = json.loads(line)
            datas.append(json_data)
    print('Finish loading data!')


    # bm25_retriever

    bm25_documents = []
    for data in datas[:1000]:
        bm25_documents.append(data['article_info'])
    # åˆ†è¯å¹¶æ„å»ºBM25æ¨¡å‹
    # tokenized_texts = [list(jieba.cut(text)) for text in bm25_documents]
    # tokenized_texts = [text.split() for text in bm25_documents]
    tokenized_texts = [mixed_segment(text) for text in bm25_documents]
    
    bm25 = BM25Okapi(tokenized_texts)
    
    if os.path.exists(os.path.join("data", "all_data.index")):
        print("Loading existing index.")
        index = read_index(os.path.join("data", "all_data.index"))
        print("Index loaded.")
        
    else:
        documents = []
        for data in datas:
            if len(data['article_info']) > 8192:
                data['article_info'] = data['article_info'][:8192]
            documents.append(data['article_info'])
        print('------------')
        index = faiss.IndexFlatL2(768)
        for i in range(13):
            print(i*10000,(i+1)*10000)
            embeddings = embedding_model.encode(documents[i*10000:(i+1)*10000])
            index.add(embeddings)
        print("The total index number is:", index.ntotal)
        print("Writing index to file system...")
        write_index(index, os.path.join("data", "all_data.index"))
        print("Index file saved.")

    return index, datas, bm25
    # return index, datas


def mixed_segment(text):
    # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
    words = jieba.cut(text)
    # å°†åˆ†è¯ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨
    words = list(words)
    
    # åˆ›å»ºä¸€ä¸ªå­˜å‚¨åˆ†è¯ç»“æœçš„åˆ—è¡¨
    segmented_words = []
    
    for word in words:
        # å¦‚æœæ˜¯è‹±æ–‡æˆ–æ•°å­—ï¼Œä½¿ç”¨nltkçš„word_tokenizeè¿›è¡Œåˆ†è¯
        if word == ' ':
            pass
        elif word.isalpha():
            segmented_words.extend(word_tokenize(word))
        else:
            segmented_words.append(word)
    
    return segmented_words


@st.cache_resource
def load_embedding_model():
    with st.spinner('åµŒå…¥æ¨¡å‹åŠ è½½ä¸­...'):
        embedding_model = SentenceTransformer("jinaai/jina-embeddings-v2-base-zh", trust_remote_code=True)
    return embedding_model


@st.cache_resource
def load_faiss_index():
    with st.spinner('å‘é‡æ•°æ®åº“åŠ è½½ä¸­...'):
        embedding_model = load_embedding_model()
        index, datas, bm25= build_faiss_index(embedding_model=embedding_model, data_path="./data/all_data.jsonl")
        
    # st.success('å‘é‡æ•°æ®åº“æ„å»ºæˆåŠŸ', icon='ğŸ’«')
    return index, datas, bm25

embedding_distance = 0.55

def get_query_embedding(embedding_model, query):
    # query_embedding = embedding_model.encode([query])[0]
    query_embedding = embedding_model.encode([query])
    return query_embedding


def get_relevant_contexts(query_embedding, query,datas, faiss_index, bm25, k: int = 3):
    score, faiss_indices = faiss_index.search(query_embedding, k)
    # print('------------------')
    # print('jina result is:', score, faiss_indices[0])
    faiss_results = [(idx, 1 / (i + 1)) for i, idx in enumerate(faiss_indices[0])]
    for i in faiss_indices[0]:
        print(datas[i]['author_cn'], datas[i]['author_en'], datas[i]['title_cn'], datas[i]['title_en'])
    if score[0][0] > 1.3:
        have_ans = False
    else:
        have_ans = True
    # bm25_scores = bm25.get_scores(query.split())
    bm25_scores = bm25.get_scores(mixed_segment(query))
    
    bm25_results = [(i, score) for i, score in enumerate(bm25_scores)]
    bm25_results = sorted(bm25_results, key=lambda x: x[1], reverse=True)[:0]


    for idx, _ in bm25_results:
        print(datas[idx]['author_cn'], datas[idx]['author_en'], datas[idx]['title_cn'], datas[idx]['title_en'])

    # å€’æ•°èåˆæ’åºï¼ˆRRFï¼‰
    fused_results = reciprocal_rank_fusion([faiss_results, bm25_results])
    
    # è¿”å›å€’æ•°æ’åºèåˆçš„å‰kä¸ªç»“æœ
    top_results = fused_results[:10]
    # print('-------')
    # print('final result is', [datas[idx] for idx, _ in top_results])
    return [datas[idx] for idx, _ in top_results], have_ans


def reciprocal_rank_fusion(results, k=60):
    rank_scores = {}
    for rank_list in results:
        for idx, (doc_id, score) in enumerate(rank_list):
            if doc_id not in rank_scores:
                rank_scores[doc_id] = 0
            rank_scores[doc_id] += 1 / (idx + k)

    sorted_scores = sorted(rank_scores.items(), key=lambda x: x[1], reverse=True)
    return sorted_scores[:k]


def response_each_query(query, embedding_model, index, datas, bm25):
    query_embedding = get_query_embedding(embedding_model, query)
    contexts, have_ans = get_relevant_contexts(query_embedding=query_embedding, query=query, datas=datas, faiss_index=index, bm25=bm25, k=10)
    if have_ans:
        response = generate_response(contexts)
    else:
        response = "æŠ±æ­‰ä½ æé—®çš„é—®é¢˜æˆ‘æ— æ³•è¿›è¡Œå›ç­”ï¼Œæˆ‘åªèƒ½å‘ä½ æ¨èä½ æƒ³è¦äº†è§£æŸä¸ªæ–¹å‘è€å¸ˆçš„ç›¸å…³ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼šâ€œå¯ä»¥å¸®æˆ‘æ¨èå‡ ä¸ªæœ‰å…³è‡ªç„¶è¯­è¨€å¤„ç†æ–¹å‘çš„è€å¸ˆå—ï¼Ÿâ€æˆ–â€œæˆ‘æƒ³è¦äº†è§£å‘½åå®ä½“è¯†åˆ«ç›¸å…³æ–¹å‘çš„å†…å®¹ï¼Œå¯ä»¥å¸®æˆ‘æ¨èå‡ ä¸ªè€å¸ˆå—ï¼Ÿâ€ï¼Œè¯·é‡æ–°è¿›è¡Œæé—®ã€‚"
    return response


def generate_response(context):
    response = "å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³ç ”ç©¶æ–¹å‘çš„æ•™å¸ˆçš„ä¿¡æ¯ï¼š\n"
    data = []
    for i in range(len(context)):
        res_one = ""
        one_data = {}
        # if context[i]['author_cn'] and context[i]['author_en'] and context[i]['author_email']:
        #     res_one = f"å§“åï¼š{context[i]['author_cn']} {context[i]['author_en']}ï¼Œé‚®ç®±ï¼š{context[i]['author_email']}"
        # elif context[i]['author_en'] and context[i]['author_email']:
        #     res_one = f"å§“åï¼š{context[i]['author_en']}ï¼Œé‚®ç®±ï¼š{context[i]['author_email']}"
        # elif context[i]['author_cn'] and context[i]['author_en']:
        #     res_one = f"å§“åï¼š{context[i]['author_cn']} {context[i]['author_en']}"
        # elif context[i]['author_cn']:
        #     res_one = f"å§“åï¼š{context[i]['author_cn']}"
        # else:
        #     res_one = f"å§“åï¼š{context[i]['author_en']}"
        if context[i]['author_cn'] and context[i]['author_en']:
            one_data['å§“å'] = str(context[i]['author_cn']) + ' ' + str(context[i]['author_en'])
        elif context[i]['author_en']:
            one_data['å§“å'] = str(context[i]['author_en'])
        else:
            one_data['å§“å'] = str(context[i]['author_cn'])
        
        if context[i]['author_email']:
            one_data['é‚®ç®±'] = str(context[i]['author_email'])
        
        # # index = response.find(res_one)
        # if context[i]['title_cn'] and context[i]['title_en']:
        #     res_one += f"ï¼Œç›¸å…³è®ºæ–‡ï¼š{context[i]['title_en']}ï¼ˆ{context[i]['title_cn']}ï¼‰"
        # elif context[i]['title_en']:
        #     res_one += f"ï¼Œç›¸å…³è®ºæ–‡ï¼š{context[i]['title_en']}"
        # else:
        #     res_one += f"ï¼Œç›¸å…³è®ºæ–‡ï¼š{context[i]['title_cn']}"
        if context[i]['title_cn'] and context[i]['title_en']:
            one_data['è®ºæ–‡'] = str(context[i]['title_en']) + '(' + str(context[i]['title_cn']) + ')'
        elif context[i]['title_en']:
            one_data['è®ºæ–‡'] = str(context[i]['title_en'])
        else:
            one_data['è®ºæ–‡'] = str(context[i]['title_cn'])
        
        data.append(one_data)
        # if not response.find(res_one) == -1:
        #     continue
        # response += res_one + '\n'
    
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å­—å…¸æ¥å­˜å‚¨åˆ†ç»„æ•°æ®
    grouped_data = {}

    # é€šè¿‡è€å¸ˆçš„å§“åè¿›è¡Œåˆ†ç»„
    for entry in data:
        name = entry["å§“å"]
        if name not in grouped_data:
            grouped_data[name] = { "é‚®ç®±": entry["é‚®ç®±"], "è®ºæ–‡åˆ—è¡¨": []}
        grouped_data[name]["è®ºæ–‡åˆ—è¡¨"].append(entry["è®ºæ–‡"])

    # è¾“å‡ºåˆ†ç»„åçš„ç»“æœ
    for name, info in grouped_data.items():
        response += f"å§“åï¼š{name}ï¼Œé‚®ç®±ï¼š{info['é‚®ç®±']} \n"
        response += f"ç›¸å…³è®ºæ–‡ï¼š\n"
        for paper in info["è®ºæ–‡åˆ—è¡¨"]:
            response += f" --- {paper}\n"
        response += '\n'
    return response



def call_gpt(client: Any,
             model: str,
             prompt: str,
             sys_prompt: str = "",
             n: int = 1,
             return_json: bool = False) -> str:
    """Perform a single api call with specified model and prompt."""
    if prompt is None or len(prompt) == 0:
        logger.warning("Prompt is empty!!!")
    if "gpt" in model or "deepseek" in model:
        if model in [
                "gpt-3.5-turbo", "gpt-4", "gpt-4o-2024-08-06", "gpt-4o-mini",
                "gpt-3.5-turbo-0301", "gpt-3.5-turbo-0613",
                "gpt-3.5-turbo-1106", "deepseek-chat"
        ]:
            if return_json:
                response = client.chat.completions.create(
                    model=model,
                    response_format={"type": "json_object"},
                    messages=[
                        {
                            "role": "system",
                            "content": sys_prompt,
                        },
                        {
                            "role": "user",
                            "content": prompt
                        },
                    ],
                    max_tokens=480,
                )
            else:
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                            "role": "system",
                            "content": sys_prompt,
                        },
                        {
                            "role": "user",
                            "content": prompt
                        },
                    ],
                    max_tokens=480,
                    top_p=1,
                    temperature=1,
                    n=n
                )
            if n == 1:
                msg = response.choices[0].message
                assert msg.role == "assistant", "Incorrect role returned."
                ans = msg.content
            else:
                ans = [msg.message.content for msg in response.choices]
        elif model in ["gpt-3.5-turbo-instruct"]:
            if len(sys_prompt) > 0:
                prompt = sys_prompt + prompt
            response = client.completions.create(model=model,
                                                 prompt=prompt,
                                                 max_tokens=480)
            ans = response.choices[0].text
        else:
            raise NotImplementedError
    else:
        raise NotImplementedError
    return ans




# def load_stopwords():
#     stopwords = set()
#     with open('stopwords.txt', 'r', encoding='utf-8') as f:
#         for line in f:
#             stopwords.add(line.strip())
#     return stopwords

# def chinese_tokenize(text, stopwords):
#     words = jieba.cut(text)
#     filtered_words = [word for word in words if word not in stopwords]
#     return filtered_words

# def cut_word(text):
#     return list(jieba.lcut(text))

# def bm25_search(query, k, index):
#     bm25_query = query
#     stopwords=load_stopwords()
#     query_delete_stopwords = chinese_tokenize(text=bm25_query, stopwords=stopwords)
#     bm25_query_dele_stopword = ''.join(query_delete_stopwords)
#     bm25_retriever = BM25Retriever.from_documents(index, preprocess_func=cut_word, k=k)
#     bm25_matching_documents_1 = bm25_retriever.get_relevant_documents(bm25_query_dele_stopword)
#     # bm25_data_1 = bm25_matching_documents_1[0].page_content
#     print(bm25_matching_documents_1)

# def search_faiss(query):
#     # search_results = retrieve_multiple_responses(query, k=3)
#     # output = reciprocal_rank_fusion(search_results_dict = search_results)
#     # print('-------------------')
#     # print(output)
#     result = ""
#     docs_and_scores = vector_db.similarity_search_with_relevance_scores(query=query, k=5)
#     for i in range(len(docs_and_scores)):
#         if docs_and_scores[i][0].metadata['author_cn'] and docs_and_scores[i][0].metadata['author_en'] and docs_and_scores[i][0].metadata['author_email']:
#             resstr = f"å§“åï¼š{docs_and_scores[i][0].metadata['author_cn']} {docs_and_scores[i][0].metadata['author_en']}ï¼Œ é‚®ç®±ï¼š{docs_and_scores[i][0].metadata['author_email']}"
#         elif docs_and_scores[i][0].metadata['author_en'] and docs_and_scores[i][0].metadata['author_email']:   
#             resstr = f"å§“åï¼š{docs_and_scores[i][0].metadata['author_en']}ï¼Œ é‚®ç®±ï¼š{docs_and_scores[i][0].metadata['author_email']}"
#         elif docs_and_scores[i][0].metadata['author_cn'] and docs_and_scores[i][0].metadata['author_en']:   
#             resstr = f"å§“åï¼š{docs_and_scores[i][0].metadata['author_cn']} {docs_and_scores[i][0].metadata['author_en']}"
#         else:
#             resstr = f"å§“åï¼š{docs_and_scores[i][0].metadata['author_en']}"
#         result = result + str(resstr) + '/n\n'
#     return result


deepseek_key = "sk-f3755f6e06dd445aa1fb494ff475a55a"
deepseek_url = "https://api.deepseek.com"

client = OpenAI(
    api_key=deepseek_key,
    base_url=deepseek_url,
    max_retries=100,
    timeout=20.0,
)


def main():
    st.set_page_config(page_title="æ—¦èæ…§é€š/å¤è”æ…§æ¡¥", page_icon="", layout="wide")
    st.header(":nazar_amulet: æ—¦èæ…§é€š/å¤è”æ…§æ¡¥", divider='blue')
    st.caption(':blue[æˆ‘ä»¬æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘äº¤å‰ç ”ç©¶çš„è¿æ¥å¹³å°ï¼ŒåŒå­¦å’Œè€å¸ˆä»¬åœ¨è¿™é‡Œå¯ä»¥è½»æ¾åœ°é—®è¯¢åˆ°è‡ªå·±æ„Ÿå…´è¶£çš„ã€ä¸ç ”ç©¶è¯¾é¢˜ç›¸å…³çš„å¤šæºå­¦ç§‘çš„å¤§å’–ä¿¡æ¯ã€‚]')

    
    with st.sidebar:
        st.header("çƒ­ç‚¹å¤§å’–")

    
    if "user_prompt_history" not in st.session_state:
        st.session_state["user_prompt_history"]=[]
    if "chat_answers_history" not in st.session_state:
        st.session_state["chat_answers_history"]=[]
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"]=[]
    embedding_model = load_embedding_model()
    index, datas, bm25 = load_faiss_index()
    
    prompt = st.chat_input("Enter your questions here")

    if prompt:
        with st.spinner("Generating......"):
            # PROMPT = """è¯·ä½ æ‰®æ¼”ä¸€ä¸ªæŸ¥è¯¢æ”¹å†™å™¨ï¼Œå°†ä¸‹é¢è¿™æ®µè¯æå–å‡ºä¸å­¦æœ¯ç ”ç©¶ç›¸å…³çš„å®ä½“å¹¶å°†æå–å‡ºçš„å®ä½“ç¿»è¯‘æˆè‹±æ–‡è¿”å›ï¼Œè¯·ä»…è¿”å›ç¿»è¯‘åçš„è‹±æ–‡ï¼š{} """
            PROMPT = """è¯·ä½ æ‰®æ¼”ä¸€ä¸ªç¿»è¯‘å™¨ï¼Œå°†ä¸‹é¢è¿™æ®µè¯ç¿»è¯‘æˆè‹±æ–‡ï¼Œè¯·ä»…è¿”å›ç¿»è¯‘åçš„å†…å®¹ï¼š{}"""
            query = PROMPT.format(prompt)
            # "gpt-4o-2024-08-06", gpt-4o-mini, deepseek-chat
            response = call_gpt(client=client, model="deepseek-chat", prompt=query, n=1)
            # print('ç¿»è¯‘åresponseä¸ºï¼š',response)

            # Storing the questions, answers and chat history
            # ans = search_faiss(query=prompt)
            ans = response_each_query(response, embedding_model, index, datas, bm25)
            st.session_state["user_prompt_history"].append(prompt)
            st.session_state["chat_answers_history"].append(ans)
            st.session_state["chat_history"].append((prompt,ans))


    # Displaying the chat history

    if st.session_state["chat_answers_history"]:
        for i, j in zip(st.session_state["chat_answers_history"], st.session_state["user_prompt_history"]):
            message1 = st.chat_message("user")
            # message1.write(j)
            message1.write(j.replace('\n', '  \n'))
            message2 = st.chat_message("assistant")
            # message2.write(i)
            message2.write(i.replace('\n', '  \n'))
        

# python -m streamlit run demo_app.py --server.port 6006
if __name__ == "__main__":
    main()
    # ROBUST KERNEL DESIGN FOR SSD TRACKER SSD-based object
    
    # query = 'å¤æ—¦å¤§å­¦å›¾ä¹¦é¦†æœŸåˆŠå›æº¯é¡¹ç›®ç®¡ç†'
    # embedding_model = load_embedding_model()
    # index, datas = load_faiss_index()
    # ans = response_each_query(query, embedding_model, index, datas)
    # bm25_search(query=query, k=3, index= index)

